{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ifran-rahman/Solar_Enhanced_IR/blob/colab_notebook/train_clahe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_Q45XoyDJ-Y"
      },
      "outputs": [],
      "source": [
        "# import cv2\n",
        "\n",
        "# image_path = 'IR2.png'\n",
        "\n",
        "# image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "# lab_img = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "# # Split LAB image into L, A, and B channels\n",
        "# l, a, b = cv2.split(lab_img)\n",
        "# # Apply CLAHE to the L channel\n",
        "# clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8,8))\n",
        "# clahe_img = clahe.apply(l)\n",
        "# # Combine the CLAHE enhanced L-channel with the A and B channels\n",
        "# updated_lab_img = cv2.merge((clahe_img, a, b))\n",
        "# # Convert LAB image back to RGB color space\n",
        "# clahe_img = cv2.cvtColor(updated_lab_img, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "# cv2.imwrite(\"IR2-Clahe.png\", clahe_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI_zs4_BDJ-b"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# import scripts\n",
        "# from scripts.saveResults import  *\n",
        "# from torch.utils.data import (\n",
        "#     Dataset,\n",
        "#     DataLoader,\n",
        "# )  # Gives easier dataset managment and creates mini batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb0lkhZJDJ-b",
        "outputId": "5def3332-bae0-421e-dcdc-9cfd4a51ed3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "# When running on the CuDNN backend, two further options must be set\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afE7aj2LDJ-c"
      },
      "source": [
        "Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn7ACI0NDJ-c"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SolarRadiance(Dataset):\n",
        "    def __init__(self, root_dir, labels, transform):\n",
        "        self.root_dir = root_dir\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        # self.data = self.load_dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.labels.iloc[index,0]\n",
        "        target = self.labels.iloc[index,1]\n",
        "\n",
        "        image = io.imread(img_path)\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)#, cv2.IMREAD_COLOR)\n",
        "        # image = cv2.resize(image, (60, 80))\n",
        "        image = cv2.resize(image, None, fx = 2, fy = 2, interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "        lab_img = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "        # Split LAB image into L, A, and B channels\n",
        "        l, a, b = cv2.split(lab_img)\n",
        "        # Apply CLAHE to the L channel\n",
        "        clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(3,3))\n",
        "        clahe_img = clahe.apply(l)\n",
        "        # Combine the CLAHE enhanced L-channel with the A and B channels\n",
        "        updated_lab_img = cv2.merge((clahe_img, a, b))\n",
        "        # Convert LAB image back to RGB color space\n",
        "        image = cv2.cvtColor(updated_lab_img, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "        normalized_image = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)\n",
        "        image = cv2.applyColorMap(normalized_image, cv2.COLORMAP_JET)\n",
        "\n",
        "        if self.transform:\n",
        "             image = self.transform(image)\n",
        "\n",
        "        y_label = torch.tensor(target)\n",
        "\n",
        "        return image, y_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rAQXbSUDJ-c"
      },
      "outputs": [],
      "source": [
        "def load_dataset(root_dir):\n",
        "        ds = pd.DataFrame()\n",
        "        dates = os.listdir(root_dir)\n",
        "\n",
        "        try:\n",
        "            for date in dates:\n",
        "                infrared_folder = os.path.join(root_dir, date, \"infrared\")\n",
        "                pyranometer_folder = os.path.join(root_dir, date, \"pyranometer\")\n",
        "                csv_path = os.path.join(pyranometer_folder, \"{date}.csv\".format(date=date))\n",
        "                if not os.path.exists(csv_path):\n",
        "                    print(\"Skipping date {date} because it does not have both infrared and pyranometer folders\".format(date=date))\n",
        "                    continue\n",
        "                ds_temp = getDs(infrared_folder, csv_path)\n",
        "\n",
        "                # append the dataframe in the final dataframe\n",
        "                # ds = ds.append(ds_temp)\n",
        "                ds = pd.concat([ds, ds_temp], ignore_index=True)\n",
        "\n",
        "                ds['name'] = ds['name'].apply(lambda img: os.path.join(root_dir, date, 'infrared', img))\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        return ds\n",
        "\n",
        "def getDs(path, labels):\n",
        "    pyranometer = pd.read_csv(labels)\n",
        "    images = os.listdir(path)\n",
        "\n",
        "\n",
        "    #convert column 1 to int\n",
        "    X = pyranometer.iloc[:,0].astype(int)\n",
        "\n",
        "    #convert to image names\n",
        "    pyranometer.iloc[:,0] = X.apply(lambda x: str(x) + 'IR.png')\n",
        "\n",
        "    # Filter pyranometer DataFrame based on the 'x' column\n",
        "\n",
        "    filtered_pyranometer = pyranometer[pyranometer.iloc[:,0].isin(images)]\n",
        "    # Display the result\n",
        "    filtered_pyranometer.columns = ['name', 'value']\n",
        "\n",
        "    filtered_pyranometer = filtered_pyranometer.drop_duplicates(subset='name')\n",
        "    return filtered_pyranometer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDzaxaX-DJ-d"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVb4ohEQDJ-d"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imVmxc0GDJ-d"
      },
      "outputs": [],
      "source": [
        "in_channel = 1\n",
        "batch_size = 256\n",
        "num_epochs = 50\n",
        "loss = 1 # if loss = 0 the model will be trained with RMSE loss and vice versa\n",
        "lr=0.01 # learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zmgN56xDJ-d"
      },
      "outputs": [],
      "source": [
        "result_dir = 'results'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n6CKxCtDJ-d",
        "outputId": "a72ec6bd-c207-4cfc-e6db-f4a32032ed6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'results'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aed79TtDDJ-e"
      },
      "outputs": [],
      "source": [
        "train_dir = 'C:/Users/yeara/OneDrive/Desktop/IR Regression/datasets/GIRASOL Dataset Extracted/train/'\n",
        "train_data  = load_dataset(train_dir)\n",
        "train_set = SolarRadiance(root_dir= train_dir, labels=train_data, transform = transforms.Compose([transforms.ToTensor()]))\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dir = 'C:/Users/yeara/OneDrive/Desktop/IR Regression/datasets/GIRASOL Dataset Extracted/val/'\n",
        "val_data  = load_dataset(val_dir)\n",
        "val_set = SolarRadiance(root_dir= val_dir, labels=val_data, transform = transforms.Compose([transforms.ToTensor()]))\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOLt_JkrDJ-e",
        "outputId": "308fac23-3922-480f-d6e3-e7dd9205e6ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46527\n",
            "11574\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader.dataset))\n",
        "print(len(val_loader.dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KU3b7pwDJ-e"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izga_AhSDJ-e"
      },
      "source": [
        "Inception Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o3XDdfZDJ-e",
        "outputId": "7c4b79a4-4230-4ab3-dc1a-6841e5ec9109"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNRegression(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (inception1): InceptionModule(\n",
              "    (conv1x1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (conv1x1_3x3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (conv3x3): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (conv1x1_5x5): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (conv5x5): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "    (conv1x1_after_pool): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (conv2): Conv2d(80, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (fc1): Linear(in_features=614400, out_features=64, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(InceptionModule, self).__init__()\n",
        "\n",
        "        # 1x1 convolution\n",
        "        self.conv1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "\n",
        "        # 1x1 followed by 3x3 convolution\n",
        "        self.conv1x1_3x3 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "        self.conv3x3 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
        "\n",
        "        # 1x1 followed by 5x5 convolution\n",
        "        self.conv1x1_5x5 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "        self.conv5x5 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
        "\n",
        "        # 3x3 max pooling followed by 1x1 convolution\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.conv1x1_after_pool = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Path 1: 1x1 convolution\n",
        "        path1 = self.conv1x1(x)\n",
        "\n",
        "        # Path 2: 1x1 convolution followed by 3x3 convolution\n",
        "        path2 = self.conv1x1_3x3(x)\n",
        "        path2 = self.conv3x3(path2)\n",
        "\n",
        "        # Path 3: 1x1 convolution followed by 5x5 convolution\n",
        "        path3 = self.conv1x1_5x5(x)\n",
        "        path3 = self.conv5x5(path3)\n",
        "\n",
        "        # Path 4: 3x3 max pooling followed by 1x1 convolution\n",
        "        path4 = self.maxpool(x)\n",
        "        path4 = self.conv1x1_after_pool(path4)\n",
        "\n",
        "        # Concatenate all paths along the channel dimension\n",
        "        output = torch.cat([path1, path2, path3, path4], dim=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "img_x = 120\n",
        "img_y = 160\n",
        "\n",
        "class CNNRegression(nn.Module):\n",
        "    def __init__(self, num_channels=3):\n",
        "        super(CNNRegression, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_channels, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # Inception module\n",
        "        self.inception1 = InceptionModule(16)\n",
        "\n",
        "        # Additional layers\n",
        "        self.conv2 = nn.Conv2d(80, 32, kernel_size=3, stride=1, padding=1)  # 80 = 16 + 24 + 24 + 16\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # Adjust the input size for the first fully connected layer\n",
        "        self.fc1 = nn.Linear(img_x * img_y * 32, 64)  # No downsampling, so input size remains the same\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        # Inception module\n",
        "        x = self.inception1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "model = CNNRegression()\n",
        "model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZqXMTt1ogy_",
        "outputId": "de0b6844-a8b6-4545-aecb-81b02eea5f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.2.1+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->torchviz)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->torchviz)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->torchviz)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->torchviz)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->torchviz)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->torchviz)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->torchviz)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->torchviz)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4132 sha256=8a8cae6d8b17ea05a50bba3e3742359c5b541ebadd7c9b87e84a4463274c1fb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchviz\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6rFtvIkDJ-e",
        "outputId": "0531a2f8-65b1-4b78-8670-bcd92f0d6646"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CNNRegression(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (fc1): Linear(in_features=614400, out_features=64, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# img_x = 120\n",
        "# img_y = 160\n",
        "\n",
        "# class CNNRegression(nn.Module):\n",
        "#     def __init__(self, num_channels=3):\n",
        "#         super(CNNRegression, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(num_channels, 16, kernel_size=3, stride=1, padding=1)\n",
        "#         self.relu1 = nn.ReLU()\n",
        "#         # Remove max-pooling layer\n",
        "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "#         self.relu2 = nn.ReLU()\n",
        "#         # Remove max-pooling layer\n",
        "#         # Adjust the input size for the first fully connected layer\n",
        "#         self.fc1 = nn.Linear(img_x * img_y * 32, 64)  # No downsampling, so input size remains the same\n",
        "#         self.relu3 = nn.ReLU()\n",
        "#         self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.relu1(x)\n",
        "#         # Remove max-pooling layer\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.relu2(x)\n",
        "#         # Remove max-pooling layer\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.relu3(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "# model = CNNRegression()\n",
        "# model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ef4ScnjDJ-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d44a0b50-eddb-4094-87e9-522f3d9b17ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 58.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained MobileNet model without its final classification layer\n",
        "mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "# Remove the last layer (classification layer)\n",
        "mobilenet = nn.Sequential(*list(mobilenet.children())[:-1])\n",
        "\n",
        "# Add custom layers for regression\n",
        "class LinearRegressionHead(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(LinearRegressionHead, self).__init__()\n",
        "        self.fc = nn.Linear(in_features, 1)  # Output 1 value for linear regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Append the custom regression head to the pre-trained model\n",
        "in_features = mobilenet[-1][-1].out_channels  # Get the number of output channels from the last layer\n",
        "model = nn.Sequential(\n",
        "    mobilenet,\n",
        "    nn.AdaptiveAvgPool2d(1),\n",
        "    nn.Flatten(),\n",
        "    LinearRegressionHead(in_features)\n",
        ")\n",
        "# model.to(device)\n",
        "# Example usage:\n",
        "# input_data = torch.randn(1, 3, 224, 224)  # Example input data with 3 channels (RGB) and 224x224 size\n",
        "# output = model(input_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8lnqqxnDJ-e"
      },
      "source": [
        "Loss and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKuYF-2gDJ-f"
      },
      "outputs": [],
      "source": [
        "if loss == 0:\n",
        "    # create a function (this my favorite choice)\n",
        "    def RMSELoss(yhat,y):\n",
        "        return torch.sqrt(torch.mean((yhat-y)**2))\n",
        "    criterion = RMSELoss\n",
        "else:\n",
        "    # Define the model, loss function, and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "# Create a StepLR scheduler\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.5)\n",
        "\n",
        "# set initial loswest_loss to an infinite number\n",
        "lowest_loss = float('inf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvWJ2OMsDJ-f",
        "outputId": "c63614a0-d1bf-4a93-c067-f999152b1007"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\yeara\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:35<00:00,  1.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 3227.8779\n",
            "mean_loss: 3227.877939638884 lowest_loss: inf\n",
            "---saved a model due to lower loss---\n",
            "Epoch  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:32<00:00,  1.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2620.5064\n",
            "mean_loss: 2620.506446506666 lowest_loss: 3227.877939638884\n",
            "---saved a model due to lower loss---\n",
            "Epoch  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:28<00:00,  2.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2594.7136\n",
            "mean_loss: 2594.713621388311 lowest_loss: 2620.506446506666\n",
            "---saved a model due to lower loss---\n",
            "Epoch  3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:27<00:00,  2.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2583.8841\n",
            "mean_loss: 2583.8840658768363 lowest_loss: 2594.713621388311\n",
            "---saved a model due to lower loss---\n",
            "Epoch  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:31<00:00,  1.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2550.8027\n",
            "mean_loss: 2550.8026735057 lowest_loss: 2583.8840658768363\n",
            "---saved a model due to lower loss---\n",
            "Epoch  5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:26<00:00,  2.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2443.7272\n",
            "mean_loss: 2443.727211827817 lowest_loss: 2550.8026735057\n",
            "---saved a model due to lower loss---\n",
            "Epoch  6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:21<00:00,  2.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2479.5214\n",
            "Epoch  7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:16<00:00,  2.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2478.3666\n",
            "Epoch  8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:25<00:00,  2.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2465.0290\n",
            "Epoch  9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:18<00:00,  2.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2484.1514\n",
            "Epoch  10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:31<00:00,  1.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2481.6267\n",
            "Epoch  11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:34<00:00,  1.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2476.5274\n",
            "Epoch  12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:35<00:00,  1.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2476.8965\n",
            "Epoch  13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:34<00:00,  1.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2476.5423\n",
            "Epoch  14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:34<00:00,  1.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.8472\n",
            "Epoch  15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:34<00:00,  1.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5684\n",
            "Epoch  16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:33<00:00,  1.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5695\n",
            "Epoch  17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:34<00:00,  1.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5633\n",
            "Epoch  18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:33<00:00,  1.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5583\n",
            "Epoch  19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:34<00:00,  1.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5574\n",
            "Epoch  20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:33<00:00,  1.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5571\n",
            "Epoch  21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:33<00:00,  1.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5579\n",
            "Epoch  22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:33<00:00,  1.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5584\n",
            "Epoch  23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:34<00:00,  1.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5584\n",
            "Epoch  24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:34<00:00,  1.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5584\n",
            "Epoch  25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:35<00:00,  1.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5584\n",
            "Epoch  26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:33<00:00,  1.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5584\n",
            "Epoch  27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 182/182 [01:33<00:00,  1.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 2475.5584\n",
            "Epoch  28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▋   | 121/182 [00:53<00:32,  1.87it/s]"
          ]
        }
      ],
      "source": [
        "losses = []\n",
        "\n",
        "from tqdm import tqdm\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    scheduler.step()\n",
        "    print('Epoch ',epoch)\n",
        "    for i, (inputs, targets) in enumerate(tqdm(train_loader)):\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs[:,0], targets.float())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate the model on the test data\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs[:,0], targets.float())\n",
        "            total_loss += loss.item()\n",
        "        mean_loss = total_loss / len(val_loader)\n",
        "        print(f'val Loss: {mean_loss:.4f}')\n",
        "        losses.append(mean_loss)\n",
        "        if  mean_loss<lowest_loss:\n",
        "            print('mean_loss: '+str(mean_loss)+' lowest_loss: '+str(lowest_loss))\n",
        "            lowest_loss = mean_loss\n",
        "            torch.save(model.state_dict(), os.path.join(result_dir,'clahe2-3x3-bicubic-model-small.pth'))\n",
        "            print('---saved a model due to lower loss---')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oOii9qjDJ-f"
      },
      "source": [
        "Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL9VjOBiDJ-f"
      },
      "outputs": [],
      "source": [
        "# # save the losses per epoch\n",
        "# import pickle\n",
        "\n",
        "# # Save the list to a file\n",
        "# with open(result_dir+'/losses.pkl', 'wb') as file:\n",
        "#     pickle.dump(losses, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivoGQFkIDJ-f"
      },
      "outputs": [],
      "source": [
        "# # Load the list from the file\n",
        "# losses_path = os.path.join(result_dir,'losses.pkl')\n",
        "# with open(losses_path, 'rb') as file:\n",
        "#     losses = pickle.load(file)\n",
        "# print(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZZBjd-8DJ-f"
      },
      "outputs": [],
      "source": [
        "# saveLossDiagram(num_epochs,losses,result_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuj6w6dXDJ-f"
      },
      "source": [
        "Train and Shut Down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi82eTy5DJ-f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}